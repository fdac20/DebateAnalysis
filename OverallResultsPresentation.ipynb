{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Presidential Debate Analysis</h1>\n",
    "<p>By Michael Wermert, Tres James, Jordan Neely, Isaac Stone, and Winston Boyd</p>\n",
    "\n",
    "<h1>Objective</h1>\n",
    "<ul>\n",
    "    <li>The purpose of the analysis was to determine if any debate factors (use of certain words, referencing certain issues, personal attacks, etc.) correlated with the outcome of the following election.</li>\n",
    "    <li> Additionally, we would like to examine these debates over time to find any interesting patterns.</li>\n",
    "    <li><strong>The following are questions that we would like to answer.</strong></li>\n",
    "</ul>\n",
    "<ol>\n",
    "    <li>How did the winning candidates’ word choice differ from the losing candidates?</li>\n",
    "    <li>Did candidates who won the election talk about particular issues more than their opponents?</li>\n",
    "    <li>Do Ad Hominem/Personal Attacks increase a candidate’s chances of winning?</li>\n",
    "    <li>Under what circumstances are politicians more likely to use positive, neutral,and negative language? </li>\n",
    "    <li>Do candidates that use longer or shorter words on average in debates perform better or worse?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Sources</h1>\n",
    "<ul>\n",
    "    <li>We retrieved all of our transcripts for the presidential debates from the site https://debates.org/voter-education/debate-transcripts/.</li>\n",
    "    <li>Election results from https://www.archives.gov/electoral-college/1960</li>\n",
    "    <li>We also used  https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state to find the results of each election and data on the electoral college outcome</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Models and Algorithms</h1>\n",
    "<h2><ul>\n",
    "    <li>We used an expanded version of the code from Mini project 1 to extract the text from the web pages. </li>\n",
    "</ul></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import clean_html\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# we may not care about the usage of stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english') + [\n",
    " 'ut', '\\'re','.', ',', '--', '\\'s', '?', ')', ':', '(', '\\'',\n",
    " '\\\"', '-', '}', '{', '&', '|', u'\\u2014', '', '–', 'still', 'good', 'well',\n",
    "'said', 'â\\x80\\x9ci', 'gutenberg-tm', 'mr', 'project', 'one', 'uh', 'don’t',\n",
    " 'would', 'made']\n",
    "\n",
    "\n",
    "# We most likely would like to remove html markup\n",
    "def cleanHtml (html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup .get_text()\n",
    "\n",
    "# We also want to remove special characters, quotes, etc. from each word\n",
    "def cleanWord (w):\n",
    "    # r in r'[.,\"\\']' tells to treat \\ as a regular character \n",
    "    # but we need to escape ' with \\'\n",
    "    # any character between the brackets [] is to be removed \n",
    "    wn = re.sub('[,\"\\.\\'&\\|@>*;/=]', \"\", w)\n",
    "    # get rid of numbers\n",
    "    return re.sub('^[0-9\\.]*$', \"\", wn)\n",
    "       \n",
    "# define a function to get text/clean/calculate frequency\n",
    "def debate_word_dictionary_generator (URL, name1, name2, modList):\n",
    "    # first get the web page\n",
    "    r = requests .get(URL)\n",
    "    \n",
    "    # Now clean\n",
    "    # remove html markup\n",
    "    t = cleanHtml (r .text) .lower()\n",
    "    \n",
    "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
    "    wds = re .split('\\s+',t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove periods, commas, etc stuck to the edges of words\n",
    "    for i in range(len(wds)):\n",
    "        wds[i] = cleanWord (wds [i])\n",
    "        \n",
    "    name1Arr = []\n",
    "    name2Arr = []\n",
    "    switcher = 3\n",
    "            \n",
    "    for i in range(len(wds)):\n",
    "        if wds[i] == name1:\n",
    "            switcher = 1\n",
    "            \n",
    "        elif wds[i] == name2:\n",
    "            switcher = 2\n",
    "            \n",
    "        elif wds[i] in modList:\n",
    "            switcher = 3\n",
    "            \n",
    "        else:\n",
    "            if switcher == 1:\n",
    "                name1Arr.append(wds[i])\n",
    "                \n",
    "            elif switcher == 2:\n",
    "                name2Arr.append(wds[i])\n",
    "    \n",
    "    # If satisfied with results, lets go to the next step: calculate frequencies\n",
    "    # We can write a loop to create a dictionary, but \n",
    "    # there is a special function for everything in python\n",
    "    # in particular for counting frequencies (like function table() in R)\n",
    "    wf1 = Counter (name1Arr)\n",
    "    wf2 = Counter (name2Arr)\n",
    "    \n",
    "    # Remove stop words from the dictionary wf\n",
    "    for k in stop_words:\n",
    "        wf1. pop(k, None)\n",
    "        wf2. pop(k, None)\n",
    "           \n",
    "        \n",
    "    #how many regular words in the document?\n",
    "    tw1 = 0\n",
    "    for w in wf1:\n",
    "       tw1 += wf1[w]\n",
    "        \n",
    "    tw2 = 0\n",
    "    for w in wf2:\n",
    "       tw2 += wf2[w] \n",
    "    # Get ordered list\n",
    "    wfs1 = sorted (wf1 .items(), key = operator.itemgetter(1), reverse=True)\n",
    "    ml1 = min(len(wfs1),30)\n",
    "    \n",
    "    wfs2 = sorted (wf2 .items(), key = operator.itemgetter(1), reverse=True)\n",
    "    ml2 = min(len(wfs2),30)\n",
    "    \n",
    "\n",
    "    #Reverse the list because barh plots items from the bottom\n",
    "    return [(wfs1 [ 0:ml1 ] [::-1], tw1), (wfs2 [ 0:ml2 ] [::-1], tw2)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>We then used the following code to generate the bar charts.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "def plotTwoLists (wf_ee, wf_bu, title):\n",
    "    f = plt.figure (figsize=(10, 6))\n",
    "    # this is painfully tedious....\n",
    "    f .suptitle (title, fontsize=20)\n",
    "    ax = f.add_subplot(111)\n",
    "    ax .spines ['top'] .set_color ('none')\n",
    "    ax .spines ['bottom'] .set_color ('none')\n",
    "    ax .spines ['left'] .set_color ('none')\n",
    "    ax .spines ['right'] .set_color ('none')\n",
    "    ax .tick_params (labelcolor='w', top='off', bottom='off', left='off', right='off', labelsize=20)\n",
    "\n",
    "    # Create two subplots, this is the first one\n",
    "    ax1 = f .add_subplot (121)\n",
    "    plt .subplots_adjust (wspace=.5)\n",
    "\n",
    "    pos = np .arange (len(wf_ee)) \n",
    "    ax1 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pylab .yticks (pos, [ x [0] for x in wf_ee ])\n",
    "    ax1 .barh (range(len(wf_ee)), [ x [1] for x in wf_ee ], align='center')\n",
    "\n",
    "    ax2 = f .add_subplot (122)\n",
    "    ax2 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pos = np .arange (len(wf_bu)) \n",
    "    pylab .yticks (pos, [ x [0] for x in wf_bu ])\n",
    "    ax2 .barh (range (len(wf_bu)), [ x [1] for x in wf_bu ], align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><ul><li>We used the NLTKs vader classification system in order to classify how much of a speech was positive or negative.</li></ul></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mwermert/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('vader_lexicon') # one time only\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer() # or whatever you want to call it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>We then used an edited version of the the code above to get the text from each candidate.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import clean_html\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# We most likely would like to remove html markup\n",
    "def cleanHtml (html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup .get_text()\n",
    "\n",
    "# We also want to remove special characters, quotes, etc. from each word\n",
    "def cleanWord (w):\n",
    "    # r in r'[.,\"\\']' tells to treat \\ as a regular character \n",
    "    # but we need to escape ' with \\'\n",
    "    # any character between the brackets [] is to be removed \n",
    "    wn = re.sub('[,\"\\.\\'&\\|@>*;/=]', \"\", w)\n",
    "    # get rid of numbers\n",
    "    return re.sub('^[0-9\\.]*$', \"\", wn)\n",
    "\n",
    "def debate_word_list (URL, name1, name2, modList):\n",
    "    # first get the web page\n",
    "    r = requests .get(URL)\n",
    "    \n",
    "    # Now clean\n",
    "    # remove html markup\n",
    "    t = cleanHtml (r .text) .lower()\n",
    "    \n",
    "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
    "    wds = re .split('\\s+',t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove periods, commas, etc stuck to the edges of words\n",
    "    for i in range(len(wds)):\n",
    "        wds[i] = cleanWord (wds [i])\n",
    "        \n",
    "    name1Arr = []\n",
    "    name2Arr = []\n",
    "    switcher = 3\n",
    "            \n",
    "    for i in range(len(wds)):\n",
    "        if wds[i] == name1:\n",
    "            switcher = 1\n",
    "            \n",
    "        elif wds[i] == name2:\n",
    "            switcher = 2\n",
    "            \n",
    "        elif wds[i] in modList:\n",
    "            switcher = 3\n",
    "            \n",
    "        else:\n",
    "            if switcher == 1:\n",
    "                name1Arr.append(wds[i])\n",
    "                \n",
    "            elif switcher == 2:\n",
    "                name2Arr.append(wds[i])\n",
    "                \n",
    "    return [name1Arr, name2Arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><ul><li>The following code was used to calculate the average word length.</li></ul></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for key, value in total_arr1:\n",
    "    for j in range(0, value):\n",
    "        total += len(key)\n",
    "        count += 1\n",
    "    \n",
    "print('Cand1: ' + str(float(total/count)))\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "for key, value in total_arr2:\n",
    "    for j in range(0, value):\n",
    "        total += len(key)\n",
    "        count += 1\n",
    "    \n",
    "print('Cand2: ' + str(float(total/count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Neural Network</h2>\n",
    "<ul>\n",
    "    <li>A recurrent neural network (RNN) is a special type of neural network in that it contains particular perceptrons that are able to maintain a ‘memory’ of the data passed into it.</li>\n",
    "    <li>For the RNN model in this project, each input takes the first 100 words of a string of words, such as a sentence in the debate in this case. Each sentence is tokenized and each word within the sentence is mapped to an integer so that it can be fed into the network.</li>\n",
    "    <li>The model was trained on a Kaggle dataset consisting of 49,582 movie reviews with binary labels (positive sentiment or negative sentiment).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n",
    "\n",
    "<br/>\n",
    "<h2>Word Count</h2>\n",
    "<img src=\"./445pic1.PNG\" />\n",
    "<ul><li>The results of our word count analysis showed that winning and losing candidates used very similar words throughout the debates, and word choice seems to have very little correlation to who wins the election.</li> </ul>\n",
    "<img src=\"./401pic2.PNG\" />\n",
    "<ul><li>This figure breaks down the top words that the losing candidates had in common with winning candidates and those that were distinct for each side. This makes it more abundantly clear that the difference of word choice between the winners and losers is miniscule and inconclusive. The distinct words fail to prove any sort of recognizable pattern.\n",
    "</li> </ul>\n",
    "<h3>Results by Party</h3>\n",
    "<img src = \"./445pic3.PNG\"/>\n",
    "<ul><li>Overall, the two parties have used very similar words over time as well.</li></ul>\n",
    "<strong>Words Exclusive to Democrats</strong>\n",
    "<ul>\n",
    "    <li>american</li>\n",
    "    <li>i've</li>\n",
    "    <li>need</li>\n",
    "    <li>right</li>\n",
    "    <li>also</li>\n",
    "    <li>plan</li>\n",
    "</ul>\n",
    "<ul><li>Words like 'right', 'need', 'americans', and 'plan' could emphasize the Democrat's higher focus on working class citizens.</li></ul>\n",
    "<strong>Words Exclusive To Republicans</strong>\n",
    "<ul>\n",
    "    <li>say</li>\n",
    "    <li>way</li>\n",
    "    <li>like</li>\n",
    "    <li>look</li>\n",
    "    <li>government</li>\n",
    "    <li>senator</li>\n",
    "</ul>\n",
    "<h2>Sentiment Analysis</h2>\n",
    "<ul>\n",
    "    <li><strong>Winning Candidates\n",
    "{'neg': 0.086, 'neu': 0.773, 'pos': 0.142, 'compound': 1.0}\n",
    " Losing Candidates\n",
    "        {'neg': 0.083, 'neu': 0.773, 'pos': 0.144, 'compound': 1.0}</strong></li>\n",
    "    <li>The average positive, neutral, and negative percentages for both winners and losers is roughly the same. </li>\n",
    "    <li>Due to the conversations being mainly about issues such as trade and taxation, a majority of the talk is considered neutral.</li>\n",
    "    <li>Generally, if a candidate is running for a second term, they will speak a little bit more positively. If the candidate is a challenger, they may be more likely to use more negative language.</li>\n",
    "    <br/>\n",
    "    <li><p><font color='green'>Carter {'neg': 0.08, 'neu': 0.788, 'pos': 0.132, 'compound': 1.0}</font></p>\n",
    "    <p><font color='green'> Ford {'neg': 0.057, 'neu': 0.793, 'pos': 0.15, 'compound': 1.0}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='red'>Reagan {'neg': 0.092, 'neu': 0.787, 'pos': 0.12, 'compound': 0.9998}</font></p>\n",
    "    <p><font color='green'>Mondale {'neg': 0.106, 'neu': 0.751, 'pos': 0.143, 'compound': 0.9999}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='green'>Clinton {'neg': 0.096, 'neu': 0.767, 'pos': 0.136, 'compound': 0.9999}</font></p>\n",
    "    <p><font color='green'>Bush {'neg': 0.08, 'neu': 0.781, 'pos': 0.14, 'compound': 1.0}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='green'>Clinton {'neg': 0.102, 'neu': 0.754, 'pos': 0.144, 'compound': 1.0}</font></p>\n",
    "    <p><font color='red'>Dole {'neg': 0.077, 'neu': 0.782, 'pos': 0.141, 'compound': 1.0}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='green'>Bush {'neg': 0.108, 'neu': 0.721, 'pos': 0.171, 'compound': 1.0}}</font></p>\n",
    "    <p><font color='red'>Kerry {'neg': 0.107, 'neu': 0.762, 'pos': 0.131, 'compound': 0.9999}}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='green'>Obama {'neg': 0.066, 'neu': 0.787, 'pos': 0.147, 'compound': 1.0}</font></p>\n",
    "    <p><font color='green'>Romney {'neg': 0.078, 'neu': 0.78, 'pos': 0.142, 'compound': 1.0}</font></p>\n",
    "    <br/>\n",
    "    <p><font color='green'>Trump {'neg': 0.095, 'neu': 0.791, 'pos': 0.115, 'compound': 0.9997}</font></p>\n",
    "    <p><font color='green'>Biden {'neg': 0.102, 'neu': 0.794, 'pos': 0.104, 'compound': -0.9977}</font></p>\n",
    "    <br/></li>\n",
    "    <li><strong>Democrats\n",
    "{'neg': 0.086, 'neu': 0.774, 'pos': 0.14, 'compound': 1.0}\n",
    " Republicans\n",
    "        {'neg': 0.082, 'neu': 0.772, 'pos': 0.147, 'compound': 1.0} </strong></li>\n",
    "    <li>Generally speaking, the two parties use a similar sentiment in their language.</li>\n",
    "</ul>\n",
    "<h2>Word Length</h2>\n",
    "<ul>\n",
    "    <li><strong>Winning Candidates: 4.347268518729053 Losing Candidates 4.354639316239316</strong></li>\n",
    "    <li>The average word length for a candidate is about 4.35.</li>\n",
    "    <li><strong>Democrats: 4.3845054900200005 Republicans 4.317131587006303</strong></li>\n",
    "    <li>The average word length of candidates from the two parties is very similar. However, the Democrats tend to use slightly longer words than the Republicans.</li>\n",
    "    <li>However, an interesting pattern is that as time goes on, the average word length in presidential debates has decreased.</li>\n",
    "    <img src=\"./avg_word_length.png\" />\n",
    "</ul>\n",
    "<h2>Sentiment Analysis - Recurrent Neural Network</h2>\n",
    "<ul>\n",
    "    <li>Each debate is divided into two lists, one for each speaker. Each list contains all the sentences spoken by one candidate during a particular debate. The words for each sentence in each list are tokenized, mapped to integer values, and fed into the network. Values between 0 and 1 (0 for negative sentiment and 1 for positive sentiment) are recorded. In the graph below, the average sentiment of each candidate for each debate by sentence is shown below.\n",
    "        \n",
    "<ul>\n",
    "    <li>Recurrent Neural Network perceptrons have elements of 'memory' and data is processed sequentially\n",
    "    <li>Trained on IMDB movie review dataset with binary sentiment classifications - https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "    <img src=\"./mean_sentiments.png\" />\n",
    "    <li>Each value is the mean sentiment of all sentences spoken by a candidate in a debate.\n",
    "    <li>The highest mean is Gerald Ford in 1976, second debate vs. Jimmy Carter.\n",
    "    <li>The lowest mean is also Gerald Ford in 1976, in the previous debate vs. Jimmy Carter.\n",
    "    <li>Dataset context issues and generalization\n",
    "    <br/>\n",
    "\n",
    "</ul>\n",
    "        \n",
    "        \n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Issues Encountered</h1>\n",
    "<ul><li>Given the nature of our project, it became difficult to find what we will train our models with. Our debates occurred from 1960 to 2020. Naturally, commonly used language has greatly changed over this period of time, so it was difficult to find a model that would be able to respond to all of the different words that have been used throughout all of the debates.</li></ul>\n",
    "<ul><li>The lack of opinion polling data for the older elections. With the newer elections, we were able to find a decent number of opinion polls from both before and after the debates. However, for the older elections, this data was much less comprehensive. \n",
    "</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Future Areas of Research</h1>\n",
    "<ul>\n",
    "    <li>In the future, it would likely be helpful to perform this analysis on other sources of speech that a candidate gives, such as speeches at campaign rallies, media appearances, and other long form speeches. </li>\n",
    "    <li>Another interesting analysis of presidential debates could be an analysis of posture, facial expressions, and voice tones during the debate. Libraires exist to provide analysis on these factors.</li>\n",
    "    <li>While we only performed this analysis on presidential debates, we could also repeat this process for vice presidential debates and presidential primary debates.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>In Conclusion</h1>\n",
    "<ul><li><strong>While we did find some interesting patterns in the text from the debates, we ultimately did not find any strong predictors that would coorelate with election success. </strong></li>\n",
    " </ul>\n",
    " <ol>\n",
    "    <li><strong>How did the winning candidates’ word choice differ from the losing candidates?</strong><br/><br/> Generally, there were few differences between the most commonly used words for candidates that won and lost the election. <br/><br/> </li>\n",
    "    <li><strong>Did candidates who won the election talk about particular issues more than their opponents?</strong><br/><br/> Typically, candidates would respond to questions, so they would talk about the same issues around the same amount.<br/><br/></li>\n",
    "    <li><strong>Do Ad Hominem/Personal Attacks increase a candidate’s chances of winning?</strong> <br/><br/>The winning candidates tend to use slightly more negative language than losing candidates, but the difference is pretty minimal. Thus, it is likely that personal attacks do not have a huge impact on the outcome. <br/><br/></li>\n",
    "    <li><strong>Under what circumstances are politicians more likely to use positive, neutral,and negative language? </strong><br/><br/> Generally, if a candidate is running for a second term, they will speak a little bit more positively. If the candidate is a challenger, they may be more likely to use more negative language.<br/><br/> </li>\n",
    "    <li><strong>Do candidates that use longer or shorter words on average in debates perform average better or worse in debates?</strong><br/><br/> The average word length used in debates seems to have little to no effect on election results.</li>\n",
    "</ol>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
