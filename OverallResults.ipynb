{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Objective</h1>\n",
    "<ul>\n",
    "    <li>The purpose of the analysis was to determine if any debate factors (use of certain words, referencing certain issues, personal attacks, etc.) correlated with the outcome of the following election.</li>\n",
    "    <li> Additionally, we would like to examine these debates over time to find any interesting patterns.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Sources</h1>\n",
    "<ul>\n",
    "    <li>We retrieved all of our transcripts for the presidential debates from the site https://debates.org/voter-education/debate-transcripts/.</li>\n",
    "    <li>Election results from https://www.archives.gov/electoral-college/1960</li>\n",
    "    <li>We also used  https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state to find the results of each election and data on the electoral college outcome</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Models and Algorithms</h1>\n",
    "<h2><ul>\n",
    "    <li>We used an expanded version of the code from Mini project 1 to extract the text from the web pages. </li>\n",
    "</ul></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import clean_html\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# we may not care about the usage of stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english') + [\n",
    " 'ut', '\\'re','.', ',', '--', '\\'s', '?', ')', ':', '(', '\\'',\n",
    " '\\\"', '-', '}', '{', '&', '|', u'\\u2014', '', '–', 'still', 'good', 'well',\n",
    "'said', 'â\\x80\\x9ci', 'gutenberg-tm', 'mr', 'project', 'one', 'uh', 'don’t',\n",
    " 'would', 'made']\n",
    "\n",
    "\n",
    "# We most likely would like to remove html markup\n",
    "def cleanHtml (html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup .get_text()\n",
    "\n",
    "# We also want to remove special characters, quotes, etc. from each word\n",
    "def cleanWord (w):\n",
    "    # r in r'[.,\"\\']' tells to treat \\ as a regular character \n",
    "    # but we need to escape ' with \\'\n",
    "    # any character between the brackets [] is to be removed \n",
    "    wn = re.sub('[,\"\\.\\'&\\|@>*;/=]', \"\", w)\n",
    "    # get rid of numbers\n",
    "    return re.sub('^[0-9\\.]*$', \"\", wn)\n",
    "       \n",
    "# define a function to get text/clean/calculate frequency\n",
    "def debate_word_dictionary_generator (URL, name1, name2, modList):\n",
    "    # first get the web page\n",
    "    r = requests .get(URL)\n",
    "    \n",
    "    # Now clean\n",
    "    # remove html markup\n",
    "    t = cleanHtml (r .text) .lower()\n",
    "    \n",
    "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
    "    wds = re .split('\\s+',t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove periods, commas, etc stuck to the edges of words\n",
    "    for i in range(len(wds)):\n",
    "        wds[i] = cleanWord (wds [i])\n",
    "        \n",
    "    name1Arr = []\n",
    "    name2Arr = []\n",
    "    switcher = 3\n",
    "            \n",
    "    for i in range(len(wds)):\n",
    "        if wds[i] == name1:\n",
    "            switcher = 1\n",
    "            \n",
    "        elif wds[i] == name2:\n",
    "            switcher = 2\n",
    "            \n",
    "        elif wds[i] in modList:\n",
    "            switcher = 3\n",
    "            \n",
    "        else:\n",
    "            if switcher == 1:\n",
    "                name1Arr.append(wds[i])\n",
    "                \n",
    "            elif switcher == 2:\n",
    "                name2Arr.append(wds[i])\n",
    "    \n",
    "    # If satisfied with results, lets go to the next step: calculate frequencies\n",
    "    # We can write a loop to create a dictionary, but \n",
    "    # there is a special function for everything in python\n",
    "    # in particular for counting frequencies (like function table() in R)\n",
    "    wf1 = Counter (name1Arr)\n",
    "    wf2 = Counter (name2Arr)\n",
    "    \n",
    "    # Remove stop words from the dictionary wf\n",
    "    for k in stop_words:\n",
    "        wf1. pop(k, None)\n",
    "        wf2. pop(k, None)\n",
    "           \n",
    "        \n",
    "    #how many regular words in the document?\n",
    "    tw1 = 0\n",
    "    for w in wf1:\n",
    "       tw1 += wf1[w]\n",
    "        \n",
    "    tw2 = 0\n",
    "    for w in wf2:\n",
    "       tw2 += wf2[w] \n",
    "    # Get ordered list\n",
    "    wfs1 = sorted (wf1 .items(), key = operator.itemgetter(1), reverse=True)\n",
    "    ml1 = min(len(wfs1),30)\n",
    "    \n",
    "    wfs2 = sorted (wf2 .items(), key = operator.itemgetter(1), reverse=True)\n",
    "    ml2 = min(len(wfs2),30)\n",
    "    \n",
    "\n",
    "    #Reverse the list because barh plots items from the bottom\n",
    "    return [(wfs1 [ 0:ml1 ] [::-1], tw1), (wfs2 [ 0:ml2 ] [::-1], tw2)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>We then used the following code to generate the bar charts.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "def plotTwoLists (wf_ee, wf_bu, title):\n",
    "    f = plt.figure (figsize=(10, 6))\n",
    "    # this is painfully tedious....\n",
    "    f .suptitle (title, fontsize=20)\n",
    "    ax = f.add_subplot(111)\n",
    "    ax .spines ['top'] .set_color ('none')\n",
    "    ax .spines ['bottom'] .set_color ('none')\n",
    "    ax .spines ['left'] .set_color ('none')\n",
    "    ax .spines ['right'] .set_color ('none')\n",
    "    ax .tick_params (labelcolor='w', top='off', bottom='off', left='off', right='off', labelsize=20)\n",
    "\n",
    "    # Create two subplots, this is the first one\n",
    "    ax1 = f .add_subplot (121)\n",
    "    plt .subplots_adjust (wspace=.5)\n",
    "\n",
    "    pos = np .arange (len(wf_ee)) \n",
    "    ax1 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pylab .yticks (pos, [ x [0] for x in wf_ee ])\n",
    "    ax1 .barh (range(len(wf_ee)), [ x [1] for x in wf_ee ], align='center')\n",
    "\n",
    "    ax2 = f .add_subplot (122)\n",
    "    ax2 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pos = np .arange (len(wf_bu)) \n",
    "    pylab .yticks (pos, [ x [0] for x in wf_bu ])\n",
    "    ax2 .barh (range (len(wf_bu)), [ x [1] for x in wf_bu ], align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><ul><li>We used the NLTKs vader classification system in order to classify how much of a speech was positive or negative.</li></ul></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mwermert/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('vader_lexicon') # one time only\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer() # or whatever you want to call it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>We then used an edited version of the the code above to get the text from each candidate.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import clean_html\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# We most likely would like to remove html markup\n",
    "def cleanHtml (html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup .get_text()\n",
    "\n",
    "# We also want to remove special characters, quotes, etc. from each word\n",
    "def cleanWord (w):\n",
    "    # r in r'[.,\"\\']' tells to treat \\ as a regular character \n",
    "    # but we need to escape ' with \\'\n",
    "    # any character between the brackets [] is to be removed \n",
    "    wn = re.sub('[,\"\\.\\'&\\|@>*;/=]', \"\", w)\n",
    "    # get rid of numbers\n",
    "    return re.sub('^[0-9\\.]*$', \"\", wn)\n",
    "\n",
    "def debate_word_list (URL, name1, name2, modList):\n",
    "    # first get the web page\n",
    "    r = requests .get(URL)\n",
    "    \n",
    "    # Now clean\n",
    "    # remove html markup\n",
    "    t = cleanHtml (r .text) .lower()\n",
    "    \n",
    "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
    "    wds = re .split('\\s+',t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove periods, commas, etc stuck to the edges of words\n",
    "    for i in range(len(wds)):\n",
    "        wds[i] = cleanWord (wds [i])\n",
    "        \n",
    "    name1Arr = []\n",
    "    name2Arr = []\n",
    "    switcher = 3\n",
    "            \n",
    "    for i in range(len(wds)):\n",
    "        if wds[i] == name1:\n",
    "            switcher = 1\n",
    "            \n",
    "        elif wds[i] == name2:\n",
    "            switcher = 2\n",
    "            \n",
    "        elif wds[i] in modList:\n",
    "            switcher = 3\n",
    "            \n",
    "        else:\n",
    "            if switcher == 1:\n",
    "                name1Arr.append(wds[i])\n",
    "                \n",
    "            elif switcher == 2:\n",
    "                name2Arr.append(wds[i])\n",
    "                \n",
    "    return [name1Arr, name2Arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n",
    "\n",
    "<br/>\n",
    "<h2>Word Count</h2>\n",
    "<img src=\"./445pic1.PNG\" />\n",
    "<ul><li>The results of our word count analysis showed that winning and losing candidates used very similar words throughout the debates, and word choice seems to have very little correlation to who wins the election.</li> </ul>\n",
    "<img src=\"./401pic2.PNG\" />\n",
    "<ul><li>This figure breaks down the top words that the losing candidates had in common and those that were distinct for each side. This makes it more abundantly clear that the difference of word choice between the winners and losers is miniscule and inconclusive. The distinct words fail to prove any sort of recognizable pattern.\n",
    "</li> </ul>\n",
    "<h3>Results by Party</h3>\n",
    "<img src = \"./445pic3.PNG\"/>\n",
    "<ul><li>Overall, the two parties have used very similar words over time as well.</li></ul>\n",
    "<strong>Words Exclusive to Democrats</strong>\n",
    "<ul>\n",
    "    <li>american</li>\n",
    "    <li>i've</li>\n",
    "    <li>need</li>\n",
    "    <li>right</li>\n",
    "    <li>also</li>\n",
    "    <li>plan</li>\n",
    "</ul>\n",
    "<ul><li>Words like 'right', 'need', 'americans', and 'plan' could emphasize the Democrat's higher focus on working class citizens.</li></ul>\n",
    "<strong>Words Exclusive To Republicans</strong>\n",
    "<ul>\n",
    "    <li>say</li>\n",
    "    <li>way</li>\n",
    "    <li>like</li>\n",
    "    <li>look</li>\n",
    "    <li>government</li>\n",
    "    <li>senator</li>\n",
    "</ul>\n",
    "<h2>Sentiment Analysis</h2>\n",
    "<ul>\n",
    "    <li><strong>Winning Candidates\n",
    "{'neg': 0.086, 'neu': 0.773, 'pos': 0.142, 'compound': 1.0}\n",
    " Losing Candidates\n",
    "        {'neg': 0.083, 'neu': 0.773, 'pos': 0.144, 'compound': 1.0}</strong></li>\n",
    "    <li>The average positive, neutral, and negative percentages for both winners and losers is roughly the same. </li>\n",
    "    <li>Due to the conversations being mainly about issues such as trade and taxation, a majority of the talk is considered neutral.</li>\n",
    "    <li>Generally, if a candidate is running for a second term, they will speak a little bit more positively. If the candidate is a challenger, they may be more likely to use more negative language.</li>\n",
    "    <br/>\n",
    "    <li><strong>Democrats\n",
    "{'neg': 0.086, 'neu': 0.774, 'pos': 0.14, 'compound': 1.0}\n",
    " Republicans\n",
    "        {'neg': 0.082, 'neu': 0.772, 'pos': 0.147, 'compound': 1.0} </strong></li>\n",
    "    <li>Generally speaking, the two parties use a similar sentiment in their language.</li>\n",
    "</ul>\n",
    "<h2>Word Count</h2>\n",
    "<ul>\n",
    "    <li><strong>Winning Candidates: 4.347268518729053 Losing Candidates 4.354639316239316</strong></li>\n",
    "    <li>The average word length for a candidate is about 4.35.</li>\n",
    "    <li><strong>Democrats: 4.3845054900200005 Republicans 4.317131587006303</strong></li>\n",
    "    <li>The average word length of candidates from the two parties is very similar. However, the Democrats tend to use slightly longer than the Republicans.</li>\n",
    "    <li>However, an interesting pattern is that as time goes on, the average word length in presidential debates has decreased.</li>\n",
    "    <img src=\"./avg_word_length.png\" />\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Issues Encountered</h1>\n",
    "<ul><li>Given the nature of our project, it became difficult to find what we will train our models with. Our debates occurred from 1960 to 2020. Naturally, commonly used language has greatly changed over this period of time, so it was difficult to find a model that would be able to respond to all of the different words that have been used throughout all of the debates.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Future Areas of Research</h1>\n",
    "<ul>\n",
    "    <li>In the future, it would likely be helpful to perform this analysis on other sources of speech that a candidate gives, such as speeches at campaign rallies, media appearances, and other long form speeches. </li>\n",
    "    <li>Another interesting analysis of presidential debates could be an analysis of posture and facial expressions during the debate.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>In Conclusion</h1>\n",
    "<ul><li><strong>While we did find some interesting patterns in the text from the debates, we ultimately did not find any strong predictors that would coorelate with election success. </strong></li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
